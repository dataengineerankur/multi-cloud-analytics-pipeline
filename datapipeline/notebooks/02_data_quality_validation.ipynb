{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2a65323-8b5d-425a-aa59-a8b8818264f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Quality Validation Notebook\n",
    "\n",
    "This notebook performs comprehensive data quality validation using AWS Deequ framework.\n",
    "\n",
    "## Steps:\n",
    "1. Load raw data from Delta Lake\n",
    "2. Define data quality rules\n",
    "3. Run AWS Deequ validation checks\n",
    "4. Identify and flag anomalies\n",
    "5. Store validation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9031167-3e73-441c-b558-839edd552294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Spark session initialized with Delta Lake and AWS S3 support\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "\n",
    "# Retrieve AWS credentials from Databricks Secrets\n",
    "access_key = dbutils.secrets.get(\"aws-keys\", \"aws-access-key\")\n",
    "secret_key = dbutils.secrets.get(\"aws-keys\", \"aws-secret-key\")\n",
    "\n",
    "# Build the Spark session with Delta Lake and S3 support\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PsychoBunny-DataQuality\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .config(\"spark.jars.packages\", \"com.amazon.deequ:deequ:2.0.4-spark-3.4\") \\\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"fs.s3a.access.key\", access)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", secret)\n",
    "spark.conf.set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")   \n",
    "\n",
    "# logger code\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Spark session initialized with Delta Lake and AWS S3 support\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "317e50cc-3b1d-43e4-9655-62ae2468e6b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Spark session initialized with Delta Lake and Deequ support\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize Spark session with Delta Lake and Deequ support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PsychoBunny-DataQuality\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.amazon.deequ:deequ:2.0.4-spark-3.4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "logger.info(\"Spark session initialized with Delta Lake and Deequ support\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a10953-c3df-45fb-b874-e360498136ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyDeequ setup completed\n"
     ]
    }
   ],
   "source": [
    "# Setup PyDeequ\n",
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.3\"\n",
    "\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite\n",
    "\n",
    "# Data paths\n",
    "RAW_DATA_PATH = \"s3://psycho-bunny-data-lake/raw-data/\"\n",
    "\n",
    "print(\"PyDeequ setup completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e45fc53-74c3-4cec-abd2-1e36dd347baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n",
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions: 2823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calendar: 6944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "customers_df = spark.read.format(\"delta\").load(f\"{RAW_DATA_PATH}customers\")\n",
    "transactions_df = spark.read.format(\"delta\").load(f\"{RAW_DATA_PATH}transactions\")\n",
    "calendar_df = spark.read.format(\"delta\").load(f\"{RAW_DATA_PATH}calendar\")\n",
    "\n",
    "print(\"Data loaded:\")\n",
    "print(f\"Customers: {customers_df.count()}\")\n",
    "print(f\"Transactions: {transactions_df.count()}\")\n",
    "print(f\"Calendar: {calendar_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c665cdb7-3d7e-42fe-a64d-ed0b25879f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Size Check:\n",
      "Overall Status: Success\n",
      "\n",
      "Results for check: {'check_status': 'Success', 'check_level': 'Warning', 'constraint_status': 'Success', 'check': 'Customer Size Check', 'constraint_message': '', 'constraint': 'SizeConstraint(Size(None))'}\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite\n",
    "\n",
    "# Test 1: Customer Data Size Check\n",
    "check = Check(spark, CheckLevel.Warning, \"Customer Size Check\") \\\n",
    "    .hasSize(lambda size: size >= 1000)\n",
    "\n",
    "result = VerificationSuite(spark) \\\n",
    "    .onData(customers_df) \\\n",
    "    .addCheck(check) \\\n",
    "    .run()\n",
    "\n",
    "print(\"Customer Size Check:\")\n",
    "print(f\"Overall Status: {result.status}\\n\")\n",
    "\n",
    "for check_result in result.checkResults:\n",
    "    print(f\"Results for check: {check_result}\")\n",
    "    for cr in check_result:\n",
    "        if isinstance(cr, dict):\n",
    "            print(f\"  Constraint: {cr['constraint']}\")\n",
    "            print(f\"  Status:     {cr['status']}\")\n",
    "            print(f\"  Message:    {cr['message']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d600f8a1-ccf7-422e-8d37-3c0cafcc371f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /databricks/driver/dq.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /databricks/driver/dq.py\n",
    "\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite\n",
    "\n",
    "class DataQuality:\n",
    "    def __init__(self, spark, df):\n",
    "        self.spark = spark\n",
    "        self.df = df\n",
    "        self.suite = VerificationSuite(spark).onData(df)\n",
    "        self.checks = []\n",
    "\n",
    "    def add_size_check(self, min_rows, level=CheckLevel.Error):\n",
    "        self.checks.append(\n",
    "            Check(self.spark, level, f\"size_at_least_{min_rows}\").hasSize(lambda s: s >= min_rows)\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def add_not_null(self, column, level=CheckLevel.Error):\n",
    "        \"\"\"Ensure no NULLs in `column`.\"\"\"\n",
    "        self.checks.append(\n",
    "            Check(self.spark, level, f\"{column}_not_null\").isComplete(column)\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def add_unique(self, column, level=CheckLevel.Error):\n",
    "        \"\"\"Ensure all values in `column` are unique.\"\"\"\n",
    "        self.checks.append(\n",
    "            Check(self.spark, level, f\"{column}_unique\").isUnique(column)\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def run(self):\n",
    "        # attach all checks\n",
    "        for c in self.checks:\n",
    "            self.suite = self.suite.addCheck(c)\n",
    "        result = self.suite.run()\n",
    "\n",
    "        if result.status != \"Success\":\n",
    "            failures = []\n",
    "            for _, checks in result.checkResults.items():\n",
    "                for cr in checks:\n",
    "                    if cr[\"status\"] != \"Success\":\n",
    "                        failures.append(f\"{cr['constraint']}: {cr['message']}\")\n",
    "            raise AssertionError(\"Data Quality failed:\\n\" + \"\\n\".join(failures))\n",
    "\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879b994f-5f6f-4407-aedc-f6c11f4cb3f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-7178245291264403>, line 13\u001b[0m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 1) Built-in checks via Deequ\u001b[39;00m\n",
       "\u001b[1;32m      9\u001b[0m dq \u001b[38;5;241m=\u001b[39m (\n",
       "\u001b[1;32m     10\u001b[0m     DataQuality(spark, customers_df)\n",
       "\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39madd_size_check(\u001b[38;5;241m1000\u001b[39m)\n",
       "\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#.add_not_null(\"customer_id\")\u001b[39;00m\n",
       "\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;241m.\u001b[39madd_unique(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m     14\u001b[0m )\n",
       "\u001b[1;32m     15\u001b[0m dq\u001b[38;5;241m.\u001b[39mrun()   \u001b[38;5;66;03m# raises if any built-in check fails\u001b[39;00m\n",
       "\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 2) Custom logic in plain Spark for regex, lookups, etc.\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mAttributeError\u001b[0m: 'DataQuality' object has no attribute 'add_unique'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AttributeError",
        "evalue": "'DataQuality' object has no attribute 'add_unique'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'DataQuality' object has no attribute 'add_unique'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "File \u001b[0;32m<command-7178245291264403>, line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 1) Built-in checks via Deequ\u001b[39;00m\n\u001b[1;32m      9\u001b[0m dq \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     10\u001b[0m     DataQuality(spark, customers_df)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39madd_size_check(\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#.add_not_null(\"customer_id\")\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;241m.\u001b[39madd_unique(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m dq\u001b[38;5;241m.\u001b[39mrun()   \u001b[38;5;66;03m# raises if any built-in check fails\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 2) Custom logic in plain Spark for regex, lookups, etc.\u001b[39;00m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'DataQuality' object has no attribute 'add_unique'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "if '/databricks/driver' not in sys.path:\n",
    "    sys.path.insert(0, '/databricks/driver')\n",
    "\n",
    "from dq import DataQuality\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dq = (\n",
    "    DataQuality(spark, customers_df)\n",
    "    .add_size_check(1000)\n",
    "    .add_not_null(\"customer_id\")\n",
    "    .add_unique(\"customer_id\")\n",
    ")\n",
    "dq.run()   \n",
    "\n",
    "email_regex = r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'\n",
    "invalids = customers_df.filter(~col(\"EMAIL\").rlike(email_regex))\n",
    "cnt = invalids.count()\n",
    "if cnt:\n",
    "    print(f\"{cnt} invalid emails\")\n",
    "    invalids.show(truncate=False)\n",
    "    raise ValueError(\"Email format validation failed\")\n",
    "\n",
    "print(\"All checks passed!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_data_quality_validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
