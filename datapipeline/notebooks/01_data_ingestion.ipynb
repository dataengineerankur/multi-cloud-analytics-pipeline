{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05dc2dbc-79f0-47c7-b2cc-7f24b909d304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Ingestion Notebook\n",
    "\n",
    "This notebook handles the ingestion of raw data from AWS S3 into the data lake.\n",
    "\n",
    "## Steps:\n",
    "1. Load customer data from multiple CSV files\n",
    "2. Load transaction data\n",
    "3. Load fiscal calendar data\n",
    "4. Perform initial data validation\n",
    "5. Store raw data in Delta Lake format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "015a485f-d128-43eb-99a6-569223c6606e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "access = dbutils.secrets.get(\"aws-keys\", key=\"aws-access-key\")\n",
    "secret = dbutils.secrets.get(\"aws-keys\", key=\"aws-secret-key\")\n",
    "\n",
    "spark.conf.set(\"fs.s3a.access.key\", access)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", secret)\n",
    "spark.conf.set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0816300e-f661-447a-9939-9cbf41224f3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Spark session initialized with Delta Lake and AWS S3 support\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import logging\n",
    "\n",
    "# Retrieve AWS credentials from Databricks Secrets\n",
    "access = dbutils.secrets.get(\"aws-keys\", \"aws-access-key\")\n",
    "secret = dbutils.secrets.get(\"aws-keys\", \"aws-secret-key\")\n",
    "\n",
    "# Build the Spark session with Delta Lake and S3 support\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"PsychoBunny-DataIngestion\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.conf.set(\"fs.s3a.access.key\", access)\n",
    "spark.conf.set(\"fs.s3a.secret.key\", secret)\n",
    "spark.conf.set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")   \n",
    "\n",
    "# logger code\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(\"Spark session initialized with Delta Lake and AWS S3 support\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c818c3-102d-4275-b3c6-eb771be1f69c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "S3_BUCKET = \"psycho-bunny-data-lake\"\n",
    "LANDING_ZONE_PATH = \"s3://{}/landing-zone/\".format(S3_BUCKET)\n",
    "RAW_DATA_PATH = \"s3://{}/raw-data/\".format(S3_BUCKET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b471d5f-58d5-4fd8-b83e-66bbb4af7baa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Python Server ready to receive messages\nINFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 unique customers\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------------------+-------------------------+---------------+--------------+-------+------------+------------+--------------------------------+---------------------------------------+--------+-----+-----+----+---------------------------------------------------------------------------------+--------------------------+-------------------------------------------------+\n|first_name|last_name|company_name                 |address                  |city           |county        |postal |phone1      |phone2      |email                           |web                                    |province|state|zip  |post|source_file                                                                      |ingestion_date            |customer_id                                      |\n+----------+---------+-----------------------------+-------------------------+---------------+--------------+-------+------------+------------+--------------------------------+---------------------------------------+--------+-----+-----+----+---------------------------------------------------------------------------------+--------------------------+-------------------------------------------------+\n|Aaron     |Kloska   |Radecker, H Philip Jr        |423 S Navajo St #56      |Brookhill      |NULL          |NULL   |07-9896-4827|0473-600-733|aaron_kloska@kloska.net.au      |http://www.radeckerhphilipjr.com.au    |NULL    |QLD  |NULL |4816|s3://psycho-bunny-data-lake/landing-zone/customers/de_shop_customers_20240701.csv|2025-06-21 22:17:18.351763|aaron_kloska_aaron_kloska@kloska.net.au          |\n|Abel      |Maclead  |Rangoni Of Florence          |37275 St  Rt 17m M       |Middle Island  |Suffolk       |NULL   |631-335-3414|631-677-3675|amaclead@gmail.com              |http://www.rangoniofflorence.com       |NULL    |NY   |11953|NULL|s3://psycho-bunny-data-lake/landing-zone/customers/de_shop_customers_20240614.csv|2025-06-21 22:17:18.351763|abel_maclead_amaclead@gmail.com                  |\n|Abraham   |Cratch   |Cavuto, John A               |41 Benedict St           |Aldborough Ward|Greater London|IG2 7QG|01599-245408|01695-305111|acratch@gmail.com               |http://www.cavutojohna.co.uk           |NULL    |NULL |NULL |NULL|s3://psycho-bunny-data-lake/landing-zone/customers/de_shop_customers_20230619.csv|2025-06-21 22:17:18.351763|abraham_cratch_acratch@gmail.com                 |\n|Adaline   |Galagher |Debbie Reynolds Hotel        |32716 N Michigan Ave #82 |Barooga        |NULL          |NULL   |02-3225-1954|0416-156-336|adaline.galagher@galagher.com.au|http://www.debbiereynoldshotel.com.au  |NULL    |NSW  |NULL |3644|s3://psycho-bunny-data-lake/landing-zone/customers/de_shop_customers_20240701.csv|2025-06-21 22:17:18.351763|adaline_galagher_adaline.galagher@galagher.com.au|\n|Adela     |Echegoyen|Fpa Corp                     |128 W Kellogg Dr         |Burnaby        |NULL          |V5B 4L5|604-571-8392|604-693-8094|adela.echegoyen@echegoyen.org   |http://www.fpacorp.com                 |BC      |NULL |NULL |NULL|s3://psycho-bunny-data-lake/landing-zone/customers/de_shop_customers_20230901.csv|2025-06-21 22:17:18.351763|adela_echegoyen_adela.echegoyen@echegoyen.org    |\n|Adelaide  |Ender    |Williams Design Group        |175 N Central Ave        |Greenslopes    |NULL          |NULL   |07-7538-5504|0473-505-816|aender@gmail.com                |http://www.williamsdesigngroup.com.au  |NULL    |QLD  |NULL |4120|s3://psycho-bunny-data-lake/landing-zone/customers/de_shop_customers_20240701.csv|2025-06-21 22:17:18.351763|adelaide_ender_aender@gmail.com                  |\n|Adelina   |Nabours  |Courtyard By Marriott        |80 Pittsford Victor Rd #9|Cleveland      |Cuyahoga      |NULL   |216-230-4892|216-937-5320|adelina_nabours@gmail.com       |http://www.courtyardbymarriott.com     |NULL    |OH   |44103|NULL|s3://psycho-bunny-data-lake/landing-zone/customers/de_shop_customers_20240614.csv|2025-06-21 22:17:18.351763|adelina_nabours_adelina_nabours@gmail.com        |\n|Adell     |Lipkin   |Systems Graph Inc Ab Dick Dlr|65 Mountain View Dr      |Whippany       |Morris        |NULL   |973-654-1561|973-662-8988|adell.lipkin@lipkin.com         |http://www.systemsgraphincabdickdlr.com|NULL    |NJ   |07981|NULL|s3://psycho-bunny-data-lake/landing-zone/customers/de_shop_customers_20240614.csv|2025-06-21 22:17:18.351763|adell_lipkin_adell.lipkin@lipkin.com             |\n|Adelle    |Nitcher  |John J Mccarthy Agency Inc   |10 Midway Dr             |Thorold        |NULL          |L2V 5C7|905-998-3758|905-848-6892|anitcher@aol.com                |http://www.johnjmccarthyagencyinc.com  |ON      |NULL |NULL |NULL|s3://psycho-bunny-data-lake/landing-zone/customers/de_shop_customers_20230901.csv|2025-06-21 22:17:18.351763|adelle_nitcher_anitcher@aol.com                  |\n|Adelle    |Schantini|Creative Photography Inc     |162 Grayson St           |Greenlands Ward|Lancashire    |FY2 0TD|01624-595660|01938-262356|adelle_schantini@yahoo.com      |http://www.creativephotographyinc.co.uk|NULL    |NULL |NULL |NULL|s3://psycho-bunny-data-lake/landing-zone/customers/de_shop_customers_20230619.csv|2025-06-21 22:17:18.351763|adelle_schantini_adelle_schantini@yahoo.com      |\n+----------+---------+-----------------------------+-------------------------+---------------+--------------+-------+------------+------------+--------------------------------+---------------------------------------+--------+-----+-----+----+---------------------------------------------------------------------------------+--------------------------+-------------------------------------------------+\nonly showing top 10 rows\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# load all CSV files with automatic schema merging\n",
    "customers_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .csv(f\"s3://{S3_BUCKET}/landing-zone/customers/*.csv\") \\\n",
    "    .withColumn(\"source_file\", input_file_name()) \\\n",
    "    .withColumn(\"ingestion_date\", current_timestamp()) \\\n",
    "    .withColumn(\"customer_id\", concat_ws(\"_\", lower(col(\"first_name\")), lower(col(\"last_name\")), lower(col(\"email\")))) \\\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    "\n",
    "print(f\"Loaded {customers_raw.count()} unique customers\")\n",
    "customers_raw.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "400b9ad8-b133-4b73-843f-3ffe89abfdea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:__main__:Loaded transaction data: 2823 records\nINFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------------+------------+---------------+------+--------+-------+-----------+--------------------+----------+--------------------+------------+-------------+-----+----------+-------+---------+---------------+----------------+--------+--------------------+--------------------+\n|ORDERNUMBER|QUANTITYORDERED|ORDERLINENUMBER|TOTAL_AMOUNT|      ORDERDATE|QTR_ID|MONTH_ID|YEAR_ID|PRODUCTCODE|        CUSTOMERNAME|     PHONE|        ADDRESSLINE1|ADDRESSLINE2|         CITY|STATE|POSTALCODE|COUNTRY|TERRITORY|CONTACTLASTNAME|CONTACTFIRSTNAME|DEALSIZE|         source_file|      ingestion_date|\n+-----------+---------------+---------------+------------+---------------+------+--------+-------+-----------+--------------------+----------+--------------------+------------+-------------+-----+----------+-------+---------+---------------+----------------+--------+--------------------+--------------------+\n|      10107|             30|              2|       -1993| 2/24/2003 0:00|     1|       2|   2021|   S10_1678|   Land of Toys Inc.|2125557818|897 Long Airport ...|        NULL|          NYC|   NY|     10022|    USA|       NA|             Yu|            Kwai|   Small|de_shop_transacti...|2025-06-21 22:17:...|\n|      10121|             34|              5|         -92|  5/7/2003 0:00|     2|       5|   2021|   S10_1678|  Reims Collectables|26.47.1555|  59 rue de l'Abbaye|        NULL|        Reims| NULL|     51100| France|     EMEA|        Henriot|            Paul|   Small|de_shop_transacti...|2025-06-21 22:17:...|\n|      10134|             41|              2|        -574|  7/1/2003 0:00|     3|       7|   2024|   S10_1678|     Lyon Souveniers|   #ERROR!|27 rue du Colonel...|        NULL|        Paris| NULL|     75508| France|     EMEA|       Da Cunha|          Daniel|  Medium|de_shop_transacti...|2025-06-21 22:17:...|\n|      10145|             45|              6|        -623| 8/25/2003 0:00|     3|       8|   2020|   S10_1678|   Toys4GrownUps.com|6265557265|  78934 Hillside Dr.|        NULL|     Pasadena|   CA|     90003|    USA|       NA|          Young|           Julie|  Medium|de_shop_transacti...|2025-06-21 22:17:...|\n|      10159|             49|             14|        -442|10/10/2003 0:00|     4|      10|   2022|   S10_1678|Corporate Gift Id...|6505551386|     7734 Strong St.|        NULL|San Francisco|   CA|      NULL|    USA|       NA|          Brown|           Julie|  Medium|de_shop_transacti...|2025-06-21 22:17:...|\n+-----------+---------------+---------------+------------+---------------+------+--------+-------+-----------+--------------------+----------+--------------------+------------+-------------+-----+----------+-------+---------+---------------+----------------+--------+--------------------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#  check transaction data\n",
    "transactions_path = \"s3://{}/landing-zone/transactions/de_shop_transactions_20230821.csv\".format(S3_BUCKET)\n",
    "\n",
    "try:\n",
    "    transactions_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(transactions_path)\n",
    "    transactions_raw = transactions_raw.withColumn(\"source_file\", lit(\"de_shop_transactions_20230821.csv\"))\n",
    "    transactions_raw = transactions_raw.withColumn(\"ingestion_date\", current_timestamp())\n",
    "    \n",
    "    logger.info(f\"Loaded transaction data: {transactions_raw.count()} records\")\n",
    "    transactions_raw.show(5)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading transactions: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e0fed0-6599-432d-822e-a4e56d4f577d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:__main__:Loaded calendar data: 6944 records\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+------------+--------------------+-------------------+-------------------+-----------------+------------------------+-----------------------+-------------------------+------------------------+-------------------+--------------+-----------+----------------+-----------------+-------------------+----------------------+---------------------+---------------------------+--------------------------+--------------------------+-------------------------+-------+------------+--------------------+\n|CALENDAR_DATE|WEEKDAY_NUMBER|WEEKDAY_NAME|FISCAL_WEEK_OF_MONTH|FISCAL_WEEK_OF_YEAR|FISCAL_MONTH_NUMBER|FISCAL_MONTH_NAME|FISCAL_FIRST_DAY_OF_WEEK|FISCAL_LAST_DAY_OF_WEEK|FISCAL_FIRST_DAY_OF_MONTH|FISCAL_LAST_DAY_OF_MONTH|FISCAL_DAY_OF_MONTH|FISCAL_QUARTER|FISCAL_YEAR|FISCAL_YEAR_WEEK|FISCAL_YEAR_MONTH|FISCAL_YEAR_QUARTER|CALENDAR_WEEK_OF_MONTH|CALENDAR_WEEK_OF_YEAR|CALENDAR_FIRST_DAY_OF_MONTH|CALENDAR_LAST_DAY_OF_MONTH|CALENDAR_FIRST_DAY_OF_YEAR|CALENDAR_LAST_DAY_OF_YEAR|WEEKEND| source_file|      ingestion_date|\n+-------------+--------------+------------+--------------------+-------------------+-------------------+-----------------+------------------------+-----------------------+-------------------------+------------------------+-------------------+--------------+-----------+----------------+-----------------+-------------------+----------------------+---------------------+---------------------------+--------------------------+--------------------------+-------------------------+-------+------------+--------------------+\n|   2010-01-31|             1|         Sun|                   1|                  1|                  1|              Feb|              2010-01-31|             2010-02-06|               2010-01-31|              2010-02-27|                  1|             1|       2010|          201001|           201001|              20101|                     5|                    4|                 2010-01-01|                2010-01-31|                2010-01-01|               2010-12-31|   true|de_dates.csv|2025-06-21 22:18:...|\n|   2010-02-01|             2|         Mon|                   1|                  1|                  1|              Feb|              2010-01-31|             2010-02-06|               2010-01-31|              2010-02-27|                  2|             1|       2010|          201001|           201001|              20101|                     1|                    5|                 2010-02-01|                2010-02-28|                2010-01-01|               2010-12-31|  false|de_dates.csv|2025-06-21 22:18:...|\n|   2010-02-02|             3|         Tue|                   1|                  1|                  1|              Feb|              2010-01-31|             2010-02-06|               2010-01-31|              2010-02-27|                  3|             1|       2010|          201001|           201001|              20101|                     1|                    5|                 2010-02-01|                2010-02-28|                2010-01-01|               2010-12-31|  false|de_dates.csv|2025-06-21 22:18:...|\n|   2010-02-03|             4|         Wed|                   1|                  1|                  1|              Feb|              2010-01-31|             2010-02-06|               2010-01-31|              2010-02-27|                  4|             1|       2010|          201001|           201001|              20101|                     1|                    5|                 2010-02-01|                2010-02-28|                2010-01-01|               2010-12-31|  false|de_dates.csv|2025-06-21 22:18:...|\n|   2010-02-04|             5|         Thu|                   1|                  1|                  1|              Feb|              2010-01-31|             2010-02-06|               2010-01-31|              2010-02-27|                  5|             1|       2010|          201001|           201001|              20101|                     1|                    5|                 2010-02-01|                2010-02-28|                2010-01-01|               2010-12-31|  false|de_dates.csv|2025-06-21 22:18:...|\n+-------------+--------------+------------+--------------------+-------------------+-------------------+-----------------+------------------------+-----------------------+-------------------------+------------------------+-------------------+--------------+-----------+----------------+-----------------+-------------------+----------------------+---------------------+---------------------------+--------------------------+--------------------------+-------------------------+-------+------------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#  fiscal calendar data\n",
    "calendar_path = \"s3://{}/landing-zone/calendar/de_dates.csv\".format(S3_BUCKET)\n",
    "\n",
    "try:\n",
    "    calendar_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(calendar_path)\n",
    "    calendar_raw = calendar_raw.withColumn(\"source_file\", lit(\"de_dates.csv\"))\n",
    "    calendar_raw = calendar_raw.withColumn(\"ingestion_date\", current_timestamp())\n",
    "    \n",
    "    logger.info(f\"Loaded calendar data: {calendar_raw.count()} records\")\n",
    "    calendar_raw.show(5)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading calendar: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ae8c2b-b746-4f0c-a286-797ac83ffa32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\nINFO:__main__:Raw data successfully stored in Delta Lake format\n"
     ]
    }
   ],
   "source": [
    "# Store raw data in Delta Lake format\n",
    "try:\n",
    "    # Store customers\n",
    "    customers_raw.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"{RAW_DATA_PATH}customers\")\n",
    "    \n",
    "    # Store transactions\n",
    "    transactions_raw.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"{RAW_DATA_PATH}transactions\")\n",
    "    \n",
    "    # Store calendar\n",
    "    calendar_raw.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"{RAW_DATA_PATH}calendar\")\n",
    "    \n",
    "    logger.info(\"Raw data successfully stored in Delta Lake format\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error storing raw data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cdff6b9-4545-4c96-922b-7d503bd87030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers loaded: 2000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions loaded: 2823\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\nINFO:__main__:Data ingestion completed successfully\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calendar loaded: 6944\nData stored at: s3://psycho-bunny-data-lake/raw-data/\n"
     ]
    }
   ],
   "source": [
    "# Data validation summary\n",
    "print(\"Customers loaded:\", customers_raw.count())\n",
    "print(\"Transactions loaded:\", transactions_raw.count())\n",
    "print(\"Calendar loaded:\", calendar_raw.count())\n",
    "print(\"Data stored at:\", RAW_DATA_PATH)\n",
    "\n",
    "\n",
    "logger.info(\"Data ingestion completed successfully\")                                       "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_data_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}